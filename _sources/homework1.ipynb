{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "The learning goals of this first hands-on sheet are: \n",
    "* to make sure that you can execute code on your machines or on Google Colab in order to experiment with LMs and RL yourself!\n",
    "* to familiarize yourself with the [HuggingFace library](https://medium.com/nlplanet/two-minutes-nlp-beginner-intro-to-hugging-face-main-classes-and-functions-fb6a1d5579c4) which provides many pretrained LMs and handy tools for working with them,\n",
    "* to develop basic intuitions about core RL concepts,\n",
    "* and to train your first RL agent!\n",
    "\n",
    "Most importantly, the homework is intended to showcase important practical aspects, provide space for learning how to find solutions to practical problems, and further conceptual understanding of the topics we discuss in class. It is *not* meant to dismay you. Therefore, even if you don't have a lot of ML / programming / technical background, you are warmly encouraged to take on the tasks, ask questions and discuss any concerns you have (with fellow students or me). There are also some hints and links to resources throughout the tasks which may help you get information which will help solving the tasks. \n",
    "\n",
    "### Homework logistics\n",
    "\n",
    "* You will have two weeks to complete the assignment (until Wed, November 8th, 12:30pm).\n",
    "* **Please do and submit your homework by yourself!**\n",
    "* However, you are warmly encouraged to ask questions and help each other, without posting full solutions, via active discussions in the dedicated Forum space on Moodle (\"Homework 1\"). Most active participants of the Forum discussions will earn some extra points for their grade!\n",
    "* Please submit your solutions via Moodle. You will find a quiz called \"Homework 1\" with questions and answer fields corresponding the respective exercise numbers listed below. \n",
    "* If you have questions or difficulties with the homework, please try to solve them with the help of your fellow students via Forum. However, I will also offer a **consultation session on Tuesday, October 31st, 2pm-4pm, on Zoom**, under the usual class link. Also don't hesitate to reach out to me via email if you have any questions, struggle or feel overwhelmed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "The exercises below will require you to execute Python code. You can do so either on your own machine, or by using [Google Colab](https://colab.research.google.com/) (free, only requires a Google account). You can easily do the latter by pressing the Colab icon at the top of the webook's page.\n",
    "You are encouraged to use the Colab option to avoid complications with local package installations etc.\n",
    "To speed up the execution of the code on Colab (especially Exercise 1), you can use the available GPU. For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n",
    "\n",
    "However, if you do want to run the code locally on your machine, I strongly encourage you to create an environment (e.g., with Conda) before you install any dependencies, and please keep in mind that pretrained language model weights might take up quite a bit of space on your hard drive or might require high RAM for prediction. In particular, the model used in these exercises requires 6GB disc space and **around 8GB RAM** for stable training.\n",
    "\n",
    "Note that the class uses PyTorch. For those of you who wish to complete final projects which include programming, you are also free to use TensorFlow for that (but I may be able to provide less support with that)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (20 points)\n",
    "\n",
    "In this exercise, we will load a pretrained LM from HuggingFace and explore how to work with it, using the tools provided by the library.\n",
    "\n",
    "### Exercise 1.1 (5 points)\n",
    "\n",
    "Your task is to use the pretrained model \"GPT-NEO\" (1.3B parameters) to *run inference*. In particular, your task is to complete the code below in order to produce a continuation for the sentence \"Reinforcement learning is \" using **beam search with k=5**. (Hint: beam-search is a particular *decoding scheme* used on top of trained language models. If you are not familiar with it, please do some research to get an overall idea about it as part of this task.) \n",
    "\n",
    "You can find information for completing the code, e.g., [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline).\n",
    "\n",
    "**TASK**: Please submit your result (i.e., produced text) on Moodle and answer questions about the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: if you are running the code on Colab, you may need to install the HuggingFace 'transformers' library\n",
    "# for that, uncomment and run the following line:\n",
    "\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import huggingface\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model='EleutherAI/gpt-neo-1.3B'\n",
    ")\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 (15 points)\n",
    "\n",
    "Your task is to complete the code below in order to *fine-tune* the model for question answering on the [\"Truthful QA\" dataset](https://huggingface.co/datasets/truthful_qa).\n",
    "The goal of this exercise is to understand the basic components that go into fine-tuning an LM from first-hand experience. Therefore, you can run the fine-tuning just for a couple of training steps. \n",
    "\n",
    "For convenience, the data loading process is already implemented for you.\n",
    "You can find relevant information for completing the task [here](https://huggingface.co/transformers/v3.3.1/training.html#fine-tuning-in-native-pytorch).\n",
    "\n",
    "**TASK**: Please post the code from the cell where you completed something on Moodle. Please answer questions about the other parts of the code on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we import the necessary libraries\n",
    "# again, use !pip install ... if libraries are missing on Colab\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = load_dataset(\"truthful_qa\", \"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a sample from the dataset to get an idea of the formatting\n",
    "print(dataset['validation'][0])\n",
    "# the dataset only has a 'validation' split, so we use that. \n",
    "# for simplicity, we are not further splitting the data into train/val/test\n",
    "# but just using everything for training\n",
    "dataset_val = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "# add padding token to tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch dataset wrapper around the huggingface dataset\n",
    "# which will allow for easy preprocessing and formatting\n",
    "class TruthfulQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Helper class to create a pytorch dataset.\n",
    "    Each sample if formatted with 'Question: {question} Answer:' prefixes.\n",
    "    Also pads and truncates the strings to a given maximum length,\n",
    "    so that they can be batched.\n",
    "    The implemented methods are required by pytorch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : huggingface dataset\n",
    "        The dataset to wrap around.\n",
    "    tokenizer : huggingface tokenizer\n",
    "        The tokenizer to use for tokenization.\n",
    "    max_length : int\n",
    "        The maximum length of the input and output sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, tokenizer, max_length=128):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single preprocessed sample from the dataset,\n",
    "        at given index idx.\n",
    "        \"\"\"\n",
    "         # NOTE: this is an updated version, compared to the initial homework\n",
    "        # source code. \n",
    "        # In particular, it includes correct attention masking and input formatting.\n",
    "        item = self.dataset[idx]\n",
    "        question = item['question']\n",
    "        answer = item['best_answer']\n",
    "        # format input\n",
    "        input = f\"Question: {question} Answer: {answer}\"\n",
    "\n",
    "        # tokenize input\n",
    "        # note that the tokenizer automatically adds SOS, EOS and PAD tokens\n",
    "        inputs = self.tokenizer(\n",
    "            input, \n",
    "            return_tensors='pt', \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset\n",
    "train_dataset = TruthfulQADataset(dataset_val, tokenizer)\n",
    "# create a DataLoader for the dataset\n",
    "# the data loader will automatically batch the data\n",
    "# and iteratively return training examples (question answer pairs) in batches\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trianing configutations \n",
    "# feel free to play around with these\n",
    "epochs  = 1\n",
    "train_steps =  100\n",
    "# using the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "# put the model in training mode\n",
    "model.train()\n",
    "# move the model to the device (e.g. GPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# define optimizer and learning rate\n",
    "optimizer = ### YOUR CODE HERE ###\n",
    "\n",
    "# define some variables to accumulate the losses\n",
    "losses = []\n",
    "\n",
    "# iterate over epochs\n",
    "for e in range(epochs):\n",
    "    # iterate over training steps\n",
    "    for i in range(train_steps):\n",
    "        # get a batch of data\n",
    "        x = next(iter(dataloader))\n",
    "        # move the data to the device (GPU)\n",
    "        x = x.to(device)\n",
    "\n",
    "        # forward pass through the model\n",
    "        ### YOUR CODE HERE ###\n",
    "        # get the loss\n",
    "        loss = ### YOUR CODE HERE ###\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        # update the parameters of the model\n",
    "        ### YOUR CODE HERE ###\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The purpose of this exercise is to just get the training running correctly. The quality of the predicted answer after the fine-tuning does not matter for the grading. That is, you don't need to worry in case the predicted answer seems not great to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "# set it to evaluation mode\n",
    "model.eval()\n",
    "model = model.to(\"cpu\")\n",
    "# generate some text for one of the questions from the dataset\n",
    "question = dataset_val[-1]['question']\n",
    "print(\"Question: \", question)\n",
    "# tokenize the question and generate an answer\n",
    "input = f\"Question: {question} Answer:\"\n",
    "input_ids = tokenizer.encode(input, return_tensors='pt').to('cpu')\n",
    "prediction = ### YOUR CODE HERE ###\n",
    "# decode the prediction\n",
    "answer = tokenizer.decode(prediction[0])\n",
    "print(\"Predicted answer after fine-tuning: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fine-tuning loss\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (10 points)\n",
    "\n",
    "The goal of this exercise is to apply basic concepts of reinforcement learning to one of the \"holy grail\" tasks in machine learning and AI -- chess.\n",
    "\n",
    "Your task is to map concepts like \"agent\", \"action\", and \"state\" that we have discussed in class onto their \"real-world\" counterparts in the game of chess (e.g., played by a computer program). \n",
    "\n",
    "**TASK:** Please fill in your responses on Moodle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (20 points) \n",
    "\n",
    "In this exercise, you will train your very first RL agent! \n",
    "\n",
    "Imagine that your agent just moved to a new town and is exploring the local restaurants. There are 10 restaurants with the names 0, 1, ..., 9 in this town.\n",
    "The agent does not know anything about the restaurants in the beginning (and also mysteriously cannot find any reviews to look at). Therefore, she needs to try the restaurants herself and try to figure out which one will make her the happiest during her time in this town (i.e., will give her the highest expected reward). \n",
    "\n",
    "This problem of trying to choose which action (i.e., going to which restaurant) is reward-maximizing in one situation, given several action options, is a (simplified) instance of a the so-called *k-armed bandit problem* (where *k* is the number of avialable actions, here: 10). This problem is very well-studied in RL.\n",
    "\n",
    "For this exercise, we assume a number of simplifications. We assume that the quality of the restaurants is deterministic (i.e., doesn't change over the times the agents goes there), and the agent's preferences don't change over time, either. (Hint: what does this mean for the value of actions and the rewards?)\n",
    "\n",
    "Based on these assumptions, in this exercise, we apply a simple algorithm estimating the *values of the available actions* $a \\in A$ at time $t$ (think: subjective value of going to the restaurant for the agent, e.g., degree of feeling happy upon eating there): \n",
    "\n",
    "$$Q_t(a)$$\n",
    "\n",
    "Specifically, we will apply a simple **sample-average** method for estimating the value of actions at time $t$ wherein the action-values are estimated as averages of the rewards that were received in the past for choosing the respective actions:\n",
    "\n",
    "$$ Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_{i | A_i = a}}{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_i = a}}$$\n",
    "\n",
    "Time $t$ here refers to the t-th time the agent is deciding which restaurant to go to in this new town. \n",
    "Based on the estimated action values, we will derive two different 'strategies of behavior' (i.e., *policies*): the greedy and the $\\epsilon$-greedy policy. \n",
    "\n",
    "**TASK:** Your task is to complete the code below to train the agent to explore the restaurants and answer some questions about the results on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this $k$-armed restaurant bandit environment, we assume that there is a ground truth value of each of the restaurants for the agent. For instance, we know that the agent's favorite food is Thai curry, and e.g. restaurant 4 has the best Thai curry in town -- therefore, restaurant 4 would have the highest true value. Formally, the true value of an action is:\n",
    "\n",
    "$$ q_*(a) = \\mathbb{E}[R_t \\mid A_t = a] $$\n",
    "\n",
    "On the other hand, the values of the other restaurants might be lower, or even negative (e.g., the agent gets food-poisoning when going there). For tpur simulation, these true values are defined below. These ground truth values are initially unknown to our agent, and her task is to estimate them from experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define possible actions (10 restaurant in the new town)\n",
    "actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# sample the true values of the actions for the agent\n",
    "true_restaurant_rewards = np.random.normal(0, 1, 10)\n",
    "true_restaurant_rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our toy world, the agent tries the different restaurants for multiple days and receives a reward (e.g., writes down her subjective happiness value) every time she went to a restaurant. A single trial is generated by the environment below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def town_environment(action, true_restaurant_rewards):\n",
    "    \"\"\"\n",
    "    The town environment returns 'an experience' of our agent,\n",
    "    i.e., the reward associated with a given action.\n",
    "    \"\"\"\n",
    "    reward = true_restaurant_rewards[action]\n",
    "    return reward\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to complete the code below so as to implement the estimation of the action values based on past experiences. In particular, your task is to implement an algorith estimating the values of actions based on accumulating experience, and track the expected reward (i.e., mean reward) that the agent would receive if she behaved according to her estimates given the particular amount of experience. \n",
    "\n",
    "Specifically, the function below should implement the *sample-average* estimation (defined above) and return an action according to the current estimate. For action selection, please implement two policies:\n",
    "* a greedy policy (returning the action with the highest value according to the current estimate)\n",
    "* an $\\epsilon$-greedy policy (returning the action with the highest value according to the current estimate in 1-$\\epsilon$ proportions of the decisions, and returning a randomly chosen action in $\\epsilon$ proportion of cases). You are free to choose your own value of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_average_estimator(old_actions, old_rewards, actions, epsilon=0):\n",
    "    \"\"\"\n",
    "    Implement the sample-average estimator of the action values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    old_actions : numpy array\n",
    "        The actions taken by the agent before the current step.\n",
    "    old_rewards : numpy array\n",
    "        The rewards received by the agent before the current step\n",
    "        for the taken actions.\n",
    "    epsilon: float\n",
    "        The probability of taking a random action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    values : numpy array\n",
    "        The estimated values of the actions.\n",
    "    best_action : int\n",
    "        The best action according to the estimated values and the \n",
    "        current policy.\n",
    "    \"\"\"\n",
    "    # initially, the values of all actions are 0\n",
    "    values = np.array(###YOUR CODE HERE###)\n",
    "    # compute averages over previously observed rewards \n",
    "    # for each action\n",
    "    for action in actions:\n",
    "        old_indices = np.where(old_actions == action)[0]\n",
    "        if len(old_indices) == 0:\n",
    "            value = 0\n",
    "        else:\n",
    "            ###YOUR CODE HERE###\n",
    "        values[action] = ###YOUR CODE HERE###\n",
    "    # return a random action with probability epsilon\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        ###YOUR CODE HERE###\n",
    "    # return the action with the highest value with random tie-breaking with probability 1 - epsilon\n",
    "    else:\n",
    "        if np.sum(values == np.max(values)) > 1:\n",
    "            best_action = np.random.choice(np.where(values == np.max(values))[0], 1)[0]\n",
    "        else:\n",
    "            ###YOUR CODE HERE###\n",
    "            \n",
    "    # return the actions' updated values\n",
    "    # and best action\n",
    "    return values, best_action\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell embeds the function into a loop where the agent gathers experiences over 90 days (i.e., over 90 action-reward pairs) and we can observe how her average reward as well as her action choices develop with accumulated experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm\n",
    "old_actions = np.array([])\n",
    "old_rewards = np.array([])\n",
    "# initialize some variables for logging\n",
    "actions_log = []\n",
    "rewards_list = []\n",
    "average_rewards_list = []\n",
    "# itentify the ground truth optimal action so as to check\n",
    "# how often the agent would choose it\n",
    "optimal_action = actions[np.argmax(true_restaurant_rewards)]\n",
    "\n",
    "# iterate over 90 \"experience steps\"\n",
    "for i in range(90):\n",
    "    # run the algorithm with a GREEDY policy\n",
    "    \n",
    "    # return selected action according to current estimates\n",
    "    values, best_action = ### YOUR CODE HERE ###\n",
    "    # observe the reward for the currently estimated best action\n",
    "    reward = town_environment(best_action,true_restaurant_rewards)\n",
    "\n",
    "    # create experience arrays\n",
    "    old_actions = np.append(old_actions, best_action)\n",
    "    old_rewards = np.append(old_rewards, reward)\n",
    "\n",
    "    # log the results\n",
    "    # check if the best action is the optimal action\n",
    "    actions_log.append(best_action == optimal_action)\n",
    "    rewards_list.append(reward)\n",
    "    average_rewards_list.append(sum(rewards_list) / len(rewards_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "plt.plot(np.cumsum(actions_log) / np.arange(1, len(actions_log) + 1))\n",
    "plt.xlabel(\"Experience steps\")\n",
    "plt.ylabel(\"Optimal action rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_rewards_list)\n",
    "plt.xlabel(\"Experience steps\")\n",
    "plt.ylabel(\"Average reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW RUN THE SAME ALGORITHM WITH EPSILON-GREEDY POLICY\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
