{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "The learning goals for the second hands-on sheet is to gain practical experience with the concepts we discussed throughout the recent sessions in class. In particular, the exercise will focus on:\n",
    "* the human feedback which goes into RLHF and provide practical questions about a publich RLHF dataset which is commonly used for finetuning LLMs\n",
    "* actually trying to finetune a small language model (GPT-2) with reinfrocement learning\n",
    "* evaluating the fine-tuned model on common benchmark tasks.\n",
    "\n",
    "Again, the homework is intended to showcase important practical aspects, further conceptual understanding of the topics we discuss in class and provide practical tools and exercise for your own future work. \n",
    "It is *not* meant to dismay you. Therefore, even if you don't have a lot of ML / programming / technical background, you are warmly encouraged to take on the tasks, ask questions and discuss any concerns you have (with fellow students or me). There are also some hints and links to resources throughout the tasks which may help you get information which will help solving the tasks. \n",
    "\n",
    "Some of the linked resources include, e.g., libraries or links to functions from libraries which may implement some of the tasks that are included in this homework. However, the provided started code intentionally spells out many steps \"by hand\" rather than using convenience functions from libraries. This is also meant to help you becom familiar with critical computation steps which might be hidden behind such libraries.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework logistics\n",
    "\n",
    "* You will have two weeks to complete the assignment (**until Saturday, December 23rd, 6pm German time**).\n",
    "* **Please do and submit your homework by yourself!**\n",
    "* However, you are warmly encouraged to ask questions and help each other, without posting full solutions, via active discussions in the dedicated Forum space on Moodle (\"Homework 2\"). Most active participants of the Forum discussions will earn some extra points for their grade!\n",
    "* Please submit your solutions via Moodle. You will find a quiz called \"Homework 2\" with questions and answer fields corresponding the respective exercise numbers listed below. \n",
    "* If you have questions or difficulties with the homework, please try to solve them with the help of your fellow students via Forum. However, don't hesitate to reach out to me via email if you have any questions, struggle or feel overwhelmed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "The exercises below will require you to execute Python code. You can do so either on your own machine, or by using [Google Colab](https://colab.research.google.com/) (free, only requires a Google account). You can easily do the latter by pressing the Colab icon at the top of the webook's page.\n",
    "You are encouraged to use the Colab option to avoid complications with local package installations etc.\n",
    "To speed up the execution of the code on Colab (especially Exercise 2 and 3), you can use the available GPU. For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n",
    "\n",
    "However, if you do want to run the code locally on your machine, I strongly encourage you to create an environment (e.g., with Conda) before you install any dependencies, and please keep in mind that pretrained language model weights might take up quite a bit of space on your hard drive or might require high RAM for prediction. \n",
    "\n",
    "Note that the class uses PyTorch. For those of you who wish to complete final projects which include programming, you are also free to use TensorFlow for that (but I may be able to provide less support with that).\n",
    "\n",
    "**Exercise 2: Note on saving your model trained on Colab**\n",
    "\n",
    "Please note that you may want to save your fine-tuned model in exercise 2 in order to be able to re-use it later. Improtantly, sessions on Colab only persist information (including files saved to the session drive) as long as your runtime is connected. Therefore, please download your model to your local machine or mount Colab to your Google drive (see instructions in Exercise 2)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (10 points)\n",
    "\n",
    "In this exercise, we will look at the aspect of human feedback in the RLHF pipeline which we discussed from a more theoretical perspective in course sessions.\n",
    "\n",
    "Your job for this exercise is to inspect an open-source human-feedback dataset provided by researchers from Anthropic. \n",
    "\n",
    "**TASK:**\n",
    "\n",
    "1. Load the following dataset from Huggingface: `Anthropic/hh-rlhf` (you can choose how to access the dataset as you wish)\n",
    "2. Understand the structure of the dataset\n",
    "3. Answer questions about the dataset and some samples from it on Moodle\n",
    "\n",
    "**Hints and helpful materials:**\n",
    "* slides from [session 4](https://polina-tsvilodub.github.io/RL4-language-model-training/04a-PG-RMs.pdf) discuss human feedback in the RLHF context\n",
    "* [blogpost](https://huggingface.co/blog/rlhf) on RLHF "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (25 points)\n",
    "\n",
    "In this task, we will fine-tuned our very own LM with reinforcement learning!\n",
    "\n",
    "We will use reinforcement learning to fine-tune a pretrained language model for the task of *positive review generation*. Specifically, we will fine-tune an LM for generating a positive movie review continuation based on a partial reivew provided as input. The continuation should be positive even if the input was negative. For this, we will use the [IMDB dataset]([imdb](https://huggingface.co/datasets/imdb?row=5)) of movie reviews (we will only use a half of the train split to speed up training). The task is inspired by the IMDB task from [this paper](https://arxiv.org/pdf/2210.01241.pdf).\n",
    "For example, we want the fine-tuned model to do the following:\n",
    "\n",
    "Example input: \"I would put this at the top of my list of films \"\n",
    "\n",
    "Example model prediction we want: \"which I would recoomend to all of my friends. Great movie!\"\n",
    " \n",
    "\n",
    "For this exercise, your task is to implement a prominent policy-gradient algorithm  -- REINFORCE [(Williams, 1992)](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf). This was one of the first algorithms introduced in the literature in the area of policy gradient methods which preceded more advanced methods like PPO we have seen in the lecture. Versions of REINFORCE are still used today; e.g., the Sparrow model which was introduced in class was trained with REINFORCE with a baseline. Similar to other policy gradient methods, it allows us to directly learn a parameterized policy following which will maximize expected returns, without learning value functions.\n",
    "\n",
    "The REINFORCE weight update rule provides a mechanism for updating parameters of the policy in order to maximize expected returns in the following way:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha \\; R\\; \\nabla \\log \\pi(a \\mid s)  $$\n",
    "\n",
    "where $\\theta_{t}$ are the current policy parameters, $\\alpha$ is a learning rate, $R$ is the reward for the current episode, $\\pi$ the current policy and $a$ is the action taken in the state $s$ (for this exercise we assume a bandit environment). Sometimes, for variance reduction purposes, a reward baseline $b$ is used and $(R-b)$ is used instead of $R$.\n",
    "Note that REINFORCE also allows us to learn the policy given rollouts under its current parameterization (i.e., we use $\\log P(a \\mid s)$ under the current policy). In other words, we approximate the true gradient of the expected return with respect to the policy parameters via *sampling*. Since we focus on episodic tasks (i.e., sequential tasks which end when a goal state is reached; in our bandit-environment case, we only have one state, so this observation is trivial) and use returns for complete rollouts, REINFORCE is a also categorized as a Monte-Carlo algorithm (no need to worry about this if you are not familiar with them). \n",
    "\n",
    "\n",
    "We will use REINFORCE to fine-tune GPT-2 which already was subjected to supervised fine-tuning for predicting reviews on the IMDB dataset (available on HuggingFace): `lvwerra/gpt2-imdb`\n",
    "\n",
    "As a reward function, we will use a pretrained sentiment classifier based on the DistilBERT architecture, also trained on the IMDB dataset (available on HuggingFace): `lvwerra/distilbert-imdb`. \n",
    "We assume that the classifier provides \"ground truth\" labels of the sentiment of IMDB reviews by providing the scores of each review being positive (1) or negative (0). For each sample, the classifier provides scores for both labels, which can then be transformed to probabilities of each label being the true one for a given sample. You can find example outputs of he classifier below. \n",
    "\n",
    "Since want our policy to predict positive reviews, we can **use the scores of the *positive* label as the reward** where higher scores mean more positive reviews, i.e., better performance.\n",
    "\n",
    "For your convenience, some boilerplate code is already provided below. \n",
    "\n",
    "**YOUR TASK**:\n",
    "1) familiarize yourself with the dataset, the models and the provided code\n",
    "2) implement the REINFORCE update rule by completing the code\n",
    "3) implement the reward computation with the classifier by completing the code\n",
    "4) train the model for 1 epoch (e.g., on Colab)\n",
    "5) save the trained model (instructions below)\n",
    "6) submit your code and example test outputs on Moodle\n",
    "7) answer the additional questions on Moodle\n",
    "   \n",
    "**Hints and additional materials:**\n",
    "* please note that the REINFORCE update rule provides a way to update parameters so as to **maximize** the reward function (which is the objective function in case of reinforcement learning). However, standard PyTorch optimizers which we use for training *minimize* the objective function. Please take this into account in your implementation of REINFORCE.\n",
    "* you can find an example with a PyTorch implementation of using REINFORCE for a grid world navigation task [here](https://medium.com/@sofeikov/reinforce-algorithm-reinforcement-learning-from-scratch-in-pytorch-41fcccafa107)\n",
    "* a practical course on deep RL by HuggingFace, specifically focusing on REINFORCE [here](https://huggingface.co/learn/deep-rl-course/unit4/policy-gradient#the-reinforce-algorithm-monte-carlo-reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: if you are running the code on Colab, you may need to install the HuggingFace 'transformers' library\n",
    "# for that, uncomment and run the following line:\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the IMDB dataset\n",
    "imdb_ds = load_dataset(\"imdb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see the structure of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a sample from the train split of the dataset\n",
    "imdb_ds['train'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we load the pretrained models and respective tokenizers that will be used to initialize the policy and the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load policy model \n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "policy = AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "\n",
    "# Load reward model \n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before incorporating these models in a RL pipeline, below you can check what they output and and how the response is output (espceially for the reward model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an example input through the policy model just to see how it works\n",
    "\n",
    "test_txt = \"This movie is \"\n",
    "input_ids = policy_tokenizer(test_txt, return_tensors='pt')\n",
    "out = policy.generate(\n",
    "    **input_ids, \n",
    "    do_sample=True, \n",
    "    temperature=0.9, \n",
    "    max_length=20, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True, \n",
    "    renormalize_logits=True\n",
    ")\n",
    "print(\"Example prediction of the pretrained policy model: \", policy_tokenizer.decode(out.sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an example from the IMDB train split to see how the reward model works\n",
    "\n",
    "input_reward = reward_tokenizer(imdb_ds['train'][0]['text'], return_tensors='pt')\n",
    "out_reward = reward_model(**input_reward)\n",
    "print(\"Raw output format of the reward model: \", out_reward)\n",
    "# transform logits to probabilities\n",
    "reward = torch.softmax(out_reward.logits, dim=1)\n",
    "print(reward) # reward at index 1 is the probability of being positive; i.e., this can be used as the training reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a dataset is defined for convenient preprocessing and loading of IMDB texts.\n",
    "In particular, since we want to train a system to predict review continuations given partial reviews as inputs, we do not need the full reviews supplied in the IMDB dataset. The dataloader below only uses the first 64 tokens for all reviews and returns these as input for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper for the IMDB dataset which returns the tokenized text\n",
    "    and truncates / pads to a maximum length of 64 tokens.\n",
    "    This is done following the paper referenced above where the input review\n",
    "    snippets were maximally 64 tokens and then the review had to be completed\n",
    "    with a positive sentiment.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, policy_tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = policy_tokenizer\n",
    "        # following the paper referenced above, input texts are <= 64 tokens\n",
    "        self.max_len = 64\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the text from the dataset\n",
    "        text = self.dataset[idx]['text']\n",
    "        # tokenize the text\n",
    "        # and manually prepend BOS token (GPT-2 tokenizer doesn't do it somehow)\n",
    "        tokens = self.tokenizer(\n",
    "            \"<|endoftext|>\" + text, \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "          )\n",
    "        # return the tokens and the attention mask\n",
    "        return {\n",
    "            'input_ids': tokens.input_ids.squeeze().to(self.device), \n",
    "            'attention_mask': tokens.attention_mask.squeeze().to(self.device)\n",
    "          }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a helper function wrapping around our reward model which will be used during RL training in order to score the generations of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward modeling function\n",
    "\n",
    "def compute_reward(\n",
    "    reward_model, \n",
    "    reward_tokenizer, \n",
    "    sample,\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the reward, formalized as the probability of a sample being positive. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward_model: AutoModelForSequenceClassification\n",
    "        The pretrained sentiment classifier to use for computing the reward.\n",
    "    reward_tokenizer: AutoTokenizer\n",
    "        The tokenizer to use for the reward model.\n",
    "    sample: list[str]\n",
    "        List of reviews generated by the policy of length batch_size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reward: torch.Tensor\n",
    "        Tensor of rewards of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    # tokenize the sample\n",
    "    input_ids = reward_tokenizer(\n",
    "        sample, \n",
    "        truncation=True, \n",
    "        max_length=128, \n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = input_ids.to(device)\n",
    "    # get the reward model prediction \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # transform logits to probabilities, use these are reward\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # return the reward\n",
    "    return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the main training loop. Next to defining the hyperparameters and the iteration over the training data, the REINFORCE update which is used as the training signal should be implemented here.\n",
    "\n",
    "**Hint:** When you first implement and test your REINFORCE implementation, you do not need to test on the entire training dataset. Test your code on a small number of training steps, print intermediate steps etc in order to sanity-check the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainining set up\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# dataset and dataloader\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "policy_tokenizer.padding_side = \"left\"\n",
    "reward_tokenizer.padding_side = \"left\"\n",
    "policy.config.pad_token_id = policy_tokenizer.eos_token_id\n",
    "policy.generation_config.pad_token_id = policy_tokenizer.eos_token_id\n",
    "\n",
    "policy = policy.to(dtype=torch.bfloat16).to(device)\n",
    "reward_model = reward_model.to(dtype=torch.bfloat16).to(device)\n",
    "\n",
    "##### Hyperparameters #####\n",
    "num_epochs = 1\n",
    "batch_size = 4\n",
    "learning_rate = ### YOUR CODE HERE ####\n",
    "###########################\n",
    "\n",
    "# instantiate the dataloader\n",
    "dataset = ImdbDataset(imdb_ds['train'], policy_tokenizer)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# \n",
    "num_steps = len(dataset) // batch_size / 2\n",
    "print(\"Number of training steps \", num_steps)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processors for the probability distribution over next tokens for generation (i.e., sampling next action)\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(1, eos_token_id=policy_tokenizer.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "# instantiate logits processors\n",
    "logits_warper = LogitsProcessorList(\n",
    "    [\n",
    "        TemperatureLogitsWarper(0.9),\n",
    "    ]\n",
    ")\n",
    "# instantiate stopping criterion\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=112)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainining set up\n",
    "\n",
    "losses = []\n",
    "rewards_list = []\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        \n",
    "        \n",
    "        out = policy.sample(\n",
    "            batch[\"input_ids\"],\n",
    "            logits_processor=logits_processor,\n",
    "            logits_warper=logits_warper,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "        ) \n",
    "        # decode predictions for the reward model\n",
    "        out_decoded = policy_tokenizer.batch_decode(\n",
    "            out.sequences,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        # print the current sequence every 10 steps\n",
    "        if step % 10 == 0:\n",
    "          print(\"current sequence: \", out_decoded)\n",
    "\n",
    "        # below, the log probs of the generated sequences are retrieved\n",
    "        out_scores = torch.stack(out.scores).squeeze()\n",
    "        log_probs = torch.nn.functional.log_softmax(out_scores, dim=-1)\n",
    "        # reshape the tensor to shape shape (batch_size, sequence_size, vocab_size)\n",
    "        log_probs = log_probs.permute(1, 0, 2)\n",
    "        # get log probs for generated tokens only\n",
    "        sequence_ids = out.sequences[:, batch['input_ids'][0].shape[0]:]\n",
    "        log_probs_continuations_tokens = log_probs.gather(\n",
    "            dim=-1, \n",
    "            index=sequence_ids.unsqueeze(-1)\n",
    "        ).squeeze()\n",
    "        \n",
    "        # compute log probability of the sequence based on the token log probs\n",
    "        #### YOUR CODE HERE #####\n",
    "        log_probs_continuations_sentences = \n",
    "        # compute the reward with the helper function defined above\n",
    "        #### YOUR CODE HERE #####\n",
    "        rewards = \n",
    "        \n",
    "        rewards_list.append(rewards.detach().cpu())\n",
    "        # compute the loss\n",
    "        #### REINFORCE implementation (i.e., implementation of the relevant parts of the formula above here) ####\n",
    "        #### YOUR CODE HERE ######\n",
    "        loss = \n",
    "        losses.append(loss.detach().cpu())\n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print the loss\n",
    "        print(f'Epoch: {epoch}, Step: {step}, Loss: {loss.item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the trained model\n",
    "\n",
    "We will use the trained model in the next erxercise; therefore, it should be save if you want to re-use it for exercise 3 at a later point. You can save the model to your Google Drive, if you are working on Colab, or locally.\n",
    "\n",
    "\n",
    "When you execute the following cell, you will be prompted to authorize Colab to access your Drive (this is a prerequisite for using this functionality, unfortunately). Please follow the displayed instructions and then execute the following code cells. Once executed, please double-check that you Drive now indeed contains your model, so as to not loose your work.\n",
    "\n",
    "Alternatively, if you do not wish to have Colab access your Drive, you can just manually download your model. To do so, please skip the next two cells, and just execute the saving cell after. Then, navigate to the directory symbol on the left panel of Colab, right-click on the new model directory and download it. If you work on a local machine, also just execute this local saving code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR GOOGLE DRIVE & COLAB USE ONLY\n",
    "# mount Colab to Drive\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR GOOGLE DRIVE & COLAB USE ONLY\n",
    "\n",
    "# do not execute this if you don't want to save to Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "policy.save_pretrained('/content/drive/My Drive/gpt2_imdb_policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LOCAL SAVING (TO COLAB SESSION OR YOUR MACHINE)\n",
    "policy.save_pretrained('gpt2_imdb_policy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we inspect the training dynamics.\n",
    "Please answer the respective questions about the plots on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fine-tuning loss\n",
    "### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average batch rewards (i.e., average reward per training step)\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (15 points)\n",
    "\n",
    "Finally, we will get our hands dirty with evaluating LLMs which already have been trained. In this task, we will use a few tasks from one of the most-used LM benchmarks, the [SuperGLUE benchmark](https://super.gluebenchmark.com/): \n",
    "* a natural language inference (NLI) task \"rte\",  \n",
    "  * a task wherein the model has to predict whether a second sentence is entailed by the first one (i.e., predict the label 'entailment' or 'no entailment')\n",
    "* a question answering task \"boolq\",\n",
    "  * a task wherein the model has to predict an answer (yes/no) to a question, given context\n",
    "* and a sentence continuation task \"copa\".\n",
    "  * a task wherein the model has to select one of two sentences as the more plausible continuation given an input sentence.\n",
    "\n",
    "We will be using (subset of) the validation splits of the tasks for our evaluation.\n",
    "\n",
    "With the introduction of first language models like BERT, a common approach to using benchmarks like SuperGLUE was to fine-tune the pretrained model on the train split of the benchmark datasets, and then use the test splits for evaluation.\n",
    "With SOTA LLMs, it is more common to do zero- or few-shot evaluation where the model has to, e.g., predict labels or select answer options without special fine-tuning, just given instructions.\n",
    "\n",
    "We are also not going to fine-tune our model on these specific tasks. Instead, as introduced in class, we are going to compare the log probabilities of different answer options (e.g., log probabilities of \"entailment\" vs. \"no entailment\" following a pair of sentences from the RTE-task). With this method, the assumption is that a model's output prediction for a particular trial is correct iff:\n",
    "$$\\log P_{LM}(\\text{<correct label> | context}) > \\log P_{LM}(\\text{<incorrect label> | context}) $$\n",
    "\n",
    "For tasks like \"copa\" where there is no single label but instead a sentence continuation, we are going to compute the average token log probability as a single-number representation of the continuation. Here, the model's prediction will count as correct iff the average log probability of the correct continuation sentence will be higher, given the input, than for the incorrect continuation.\n",
    "We will not using task instructions in our evaluation since the model wasn't fine-tuned on instruction-following.\n",
    "\n",
    "Your job is to complete the code below, *evaluate the model which you have fine-tuned above* and summarize the results you find in a few words (see below for more detailed step-by-step instructions). If you have issues with the previous task and cannot use your own fine-tuned model, please use the initial IMDB fine-tuned GPT-2 with which we initialized the policy in exercise 2. Please indicate which model you are testing on Moodle in the respective exercise responses.\n",
    "\n",
    "**TASK:**\n",
    "1) Download the data for the three tasks by uncommenting and executing the first code cell below, or by navigating to [the repository](https://github.com/polina-tsvilodub/RL4-language-model-training/blob/main/RL4-language-model-training/data/homework2.zip) and downloading the file and unzipping it.\n",
    "2) Familiarize yourself with the code and briefly familiarize yourself with the selected tasks of the benchmark (more detailed information about the tasks can be found in the [paper](https://w4ngatang.github.io/static/papers/superglue.pdf) or just on the internet, e.g., [here](https://medium.com/nlplanet/two-minutes-nlp-superglue-tasks-and-2022-leaderboard-492d8c849ed)).\n",
    "3) Complete the code which tests the model on the benchmarks (helper for retrieveing log probability of labels is provided).\n",
    "4) Submit your completion of the code on Moodle.\n",
    "5) Submit your results on Moodle.\n",
    "6) Answer some questions about the task on Moodle.\n",
    "\n",
    "**Hints and useful materials:**\n",
    "* Note that for some of the tasks we actually pass two sentences as the context, which is then followed by the task labels. Make sure to pass both sentences, where required.\n",
    "* An example evaluation on SuperGLUE can be viewed here. Note, however, \n",
    "* An example paper comparing (negative) log probabilities of sequences under language model (i.e., using a similar metric) can be found [here](https://aclanthology.org/2021.acl-long.76.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/polina-tsvilodub/RL4-language-model-training/blob/main/RL4-language-model-training/data/homework2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip homework2.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you experience issues with the download and/or unzupping when using wget, please download the zip file manually, upload it to Colab and then execute the unzipping step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2_imdb_policy')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_of_completion(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        completion,\n",
    "):\n",
    "        \"\"\"\n",
    "        Convenience function for computing the log probability of a completion\n",
    "        given a prompt. This is used to compute the log probability of the\n",
    "        correct and incorrent labels given different trials for the different\n",
    "        SuperGLUE tasks.\n",
    "        \"\"\"\n",
    "        # tokenize the prompt and the completion \n",
    "        # truncate so as to fit into to maximal context window of gpt-2\n",
    "        # which is 1024 tokens\n",
    "        input_ids = tokenizer( \n",
    "                prompt + completion,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=1024,\n",
    "        )['input_ids'].to(device)  \n",
    "        \n",
    "        # separately tokenize prompt\n",
    "        # so as to access the logits for the completion only\n",
    "        # when scoring the completion\n",
    "        input_ids_prompt = tokenizer( \n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=1024\n",
    "        )['input_ids'].to(device) \n",
    "\n",
    "        # create attention mask and position ids\n",
    "        attention_mask = (input_ids != tokenizer.eos_token_id).to(dtype=torch.int64)\n",
    "        position_ids = attention_mask.cumsum(-1)-1\n",
    "        # get the logits for the completion\n",
    "        with torch.no_grad():\n",
    "                out = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        position_ids=position_ids\n",
    "                )\n",
    "\n",
    "        # get the logits of the completion\n",
    "        # for that, make a tensor of the logits\n",
    "        # for the completion only\n",
    "        # in particular, we shift the indices by one to the left to access logits of the \n",
    "        # actual sequence tokens\n",
    "        logits_completion = out.logits[:, :-1]\n",
    "        logits_completion = logits_completion.squeeze()\n",
    "        # get the log probabilities for the completion\n",
    "        log_probs = torch.nn.functional.log_softmax(\n",
    "                logits_completion,\n",
    "                dim=-1\n",
    "        )\n",
    "        # retrieve the logit corresponding to the actual completion tokens\n",
    "        try:\n",
    "                log_completion_tokens = log_probs.gather(\n",
    "                        dim=-1, \n",
    "                        index=input_ids[:, 1:].squeeze().unsqueeze(-1)\n",
    "                )\n",
    "        except:\n",
    "                log_completion_tokens = log_probs.gather(\n",
    "                        dim=-1, \n",
    "                        index=input_ids[:, 1:].unsqueeze(-1)\n",
    "                )\n",
    "\n",
    "        continuationConditionalLogProbs = log_completion_tokens[\n",
    "                (input_ids_prompt.shape[-1]-1):\n",
    "        ]\n",
    "        completion_log_prob = torch.mean(\n",
    "                continuationConditionalLogProbs\n",
    "        ).cpu()\n",
    "        \n",
    "        return completion_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the tasks\n",
    "\n",
    "tasks = [\"copa\", \"rte\", \"boolq\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for t in tasks:\n",
    "    print(f\"--- evaluating on task {t} ---\")\n",
    "    path = f\"homework2/super_glue_formatted_{t}.csv\"\n",
    "    # read the task data\n",
    "    df = pd.read_csv(path)\n",
    "    # iterate over the trials\n",
    "    # note that for the BoolQ and RTE tasks, the input\n",
    "    # prompt actually consists of two sentences, and the continuation\n",
    "    # is each of the labels\n",
    "    # therefore, we need to pass both sentences as the input prompt\n",
    "    # to the evaluation\n",
    "    \n",
    "    #### YOUR CODE HERE #####\n",
    "    prompt = \n",
    "    \n",
    "    # compute the log probabilities for the correct and incorrect answers\n",
    "    # for each trial in each task\n",
    "    ##### YOUR CODE HERE #####\n",
    "    get_log_prob_of_completion(\n",
    "        #### YOUR CODE HERE #####\n",
    "    )\n",
    "    \n",
    "    # evaluate resulting log probabilities\n",
    "    # i.e., compute whether the log probability of the correct answer\n",
    "    # is higher than the log probability of the incorrect answer\n",
    "    #### YOUR CODE HERE #####\n",
    "\n",
    "    # track results so that you can compute average test accuracy\n",
    "    results.append(#### YOUR CODE HERE #####)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average accuracy by task \n",
    "# #### YOUR CODE HERE #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
