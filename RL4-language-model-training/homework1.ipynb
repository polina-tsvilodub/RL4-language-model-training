{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "The learning goals of this first hands-on sheet are: \n",
    "* to make sure that you can execute code on your machines or on Google Colab in order to experiment with LMs and RL yourself!\n",
    "* to familiarize yourself with the HuggigFace library which provides many pretrained LMs and handy tools for working with them,\n",
    "* to develop basic intuitions about core RL concepts,\n",
    "* and to train your first RL agent!\n",
    "\n",
    "### Homework logistics\n",
    "\n",
    "* please submit via moodle. you will find the corresponding questions with answer fields for the respective exercise numbers there. \n",
    "* TBD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "**TODO** Conda envs, RAM, Colab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (20 points)\n",
    "\n",
    "In this exercise, we will load a pretrained LM from HuggingFace and explore how to work with it, using the tools provided by the library.\n",
    "\n",
    "### Exercise 1.1 (5 points)\n",
    "\n",
    "Your task is to use the pretrained model \"GPT-NEO\" (1.3B parameters) to *run inference*. In particular, your task is to complete the code below in order to produce a continuation for the sentence \"Reinforcement learning is \" using **beam search with k=5**. (Hint: beam-search is a particular *decoding scheme* used on top of trained language models. If you are not familiar with it, please do some research to get an overall idea about it as part of this task.) \n",
    "\n",
    "You can find information for completing the code, e.g., [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline).\n",
    "\n",
    "**TASK**: Please submit your result (i.e., produced text)  on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n",
    "## YPUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 (5 points)\n",
    "\n",
    "Your task is to complete the code below in order to *fine-tune* the model for question answering on the [\"Explain like I'm 5\" dataset](https://huggingface.co/datasets/eli5).\n",
    "The goal of this exercise is to understand the basic components that go into fine-tuning an LM from first-hand experience. Therefore, you can run the fine-tuning just for a couple of training steps. \n",
    "\n",
    "For convenience, the data loading process is already implemented for you.\n",
    "You can find relevant information for completing the task [here].\n",
    "\n",
    "**TODO decide this.** In particular, in this task we use handy wrappers for training provided by Huggingface. We could also implement this manually. \n",
    "\n",
    "**TASK**: Please post the code from the cell where you completed something on Moodle. Please answer questions about the other parts of the code on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"eli5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (10 points)\n",
    "\n",
    "The goal of this exercise is to apply basic concepts of reinforcement learning to one of the \"holy grail\" tasks in machine learning and AI -- chess.\n",
    "\n",
    "Your task is to map concepts like \"agent\", \"action\", and \"state\" that we have discussed in class onto their \"real-world\" counterparts in the game of chess (e.g., played by a computer program). \n",
    "\n",
    "**TASK:** Please fill in your responses on Moodle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (20 points) \n",
    "\n",
    "In this exercise, you will train your very first RL agent! \n",
    "\n",
    "Imagine that your agent just moved to a new town and is exploring the local restaurants. There are 10 restaurants with the names 0, 1, ..., 9 in this town.\n",
    "The agent does not know anything about the restaurants in the beginning (and also mysteriously cannot find any reviews to look at). Therefore, she needs to try the restaurants herself and try to figure out which one will make her the happiest during her time in this town (i.e., will give her the highest expected reward). \n",
    "\n",
    "This problem of trying to choose which action (i.e., going to which restaurant) is reward-maximizing in one situation, given several action options, is a (simplified) instance of a the so-called *k-armed bandit problem* (where *k* is the number of avialable actions, here: 10). \n",
    "\n",
    "For this exercise, we assume a number of simplifications. We assume that the quality of the restaurants is deterministic (i.e., doesn't change over the times the agents goes there), and the agent's preferences don't change over time, either. (Hint: what does this mean for the value of actions and the rewards?)\n",
    "\n",
    "Based on these assumptions, in this exercise, we apply a simple algorithm estimating the *values of the available actions* $a \\in A$ at time $t$ (think: subjective value of going to the restaurant for the agent, e.g., degree of feeling happy upon eating there): \n",
    "\n",
    "$$Q_t(a)$$\n",
    "\n",
    "Specifically, we will apply a simple **sample-average** method for estimating the value of actions at time $t$ wherein the action-values are estimated as averages of the rewards that were received in the past for choosing the respective actions:\n",
    "\n",
    "$$ Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_{i | A_i = a}}{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_i = a}}$$\n",
    "\n",
    "Time $t$ here refers to the t-th time the agent is deciding which restaurant to go to in this new town. \n",
    "Based on the estimated action values, we will derive two different 'strategies of behavior' (i.e., *policies*): the greedy and the epsilon-greedy policy. \n",
    "\n",
    "**TASK:** Your task is to complete the code below to train the agent to explore the restaurants and answer some questions about the results on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this $k$-armed restaurant bandit environment, we assume that there is a true value of each of the restaurants for the agent. For instance, we know that the agent's favorite food is Thai curry, and restaurant 4 has the best Thai curry in town -- therefore, restaurant 4 has the highest true value. Formally, the true value of an action is:\n",
    "\n",
    "$$ q_*(a) = \\mathbb{E}[R_t \\mid A_t = a] $$\n",
    "\n",
    "On the other hand, the values of the other restaurants might be lower, or even negative (e.g., the agent gets food-poisoning when going there). These true values are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define possible actions (10 restaurant in the new town)\n",
    "actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# generate a series ob observations\n",
    "true_restaurant_rewards = np.random.normal(0, 1, 10)\n",
    "true_restaurant_rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our toy world, the agent tries the different restaurants for 90 days and receives a reward (e.g., writes down her subjective happiness value) every time she went to a restaurant. These are generated by the environment below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def town_environment(actions, true_restaurant_rewards):\n",
    "    \"\"\"\n",
    "    The town environment returns a random action and the reward of that action\n",
    "    \"\"\"\n",
    "    action = np.random.choice(actions, 1)[0]\n",
    "    reward = true_restaurant_rewards[action]\n",
    "    return action, reward\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, below, your task is to complete the code implement estimation of the action values based on past experiences. \n",
    "\n",
    "The function should implement the *sample-average* estimation and return an action according to the current estimate. For action selection, please implement two policies:\n",
    "* a greedy policy (returning the action with the highest value according to the current estimate)\n",
    "* an $\\epsilon$-greedy policy (returning the action with the highest value according to the current estimate in 1-$\\epsilon$ proportions of the decisions, and returning a randomly chosen action in $\\epsilon$ proportion of cases). You are free to choose your own value of epsilon.\n",
    "\n",
    "The following cell embeds the function into a loop where the agent gathers experiences over 90 days (i.e., over 90 action-reward pairs) and we can observe how her average reward as well as her action choices develop with accumulated experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_average_estimator(old_actions, old_rewards, actions, rewards, epsilon=0):\n",
    "    # return the action with the highest reward with probability 1 - epsilon\n",
    "    # return a random action with probability epsilon\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    return values, best_action, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm\n",
    "old_actions = np.array([])\n",
    "old_rewards = np.array([])\n",
    "# initialize some variables for logging\n",
    "actions_log = []\n",
    "optimal_action = actions[np.argmax(true_restaurant_rewards)]\n",
    "print(\"Optimal action: \", optimal_action)\n",
    "rewards_log = 0\n",
    "rewards_log_list = []\n",
    "\n",
    "for i, r in range(90):\n",
    "    taken_action, observed_reward = town_environment(actions, true_restaurant_rewards)\n",
    "    old_actions = np.append(old_actions, taken_action)\n",
    "    old_rewards = np.append(old_rewards, observed_reward)\n",
    "    # run the algorithm\n",
    "    values, best_action, reward = sample_average_estimator(old_actions, old_rewards, actions, true_restaurant_rewards)\n",
    "    # log the results\n",
    "    actions_log.append(best_action == optimal_action)\n",
    "    rewards_log += reward\n",
    "    rewards_log_list.append(rewards_log)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
